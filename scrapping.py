import json
import requests
from bs4 import BeautifulSoup
import time
import os

def enrich_json_with_scraping(json_input_path, json_output_path):
    """
    Lee un archivo JSON con artÃ­culos, extrae el texto completo de los enlaces
    usando una estrategia de bÃºsqueda mÃºltiple, y guarda el resultado.
    """
    
    if os.path.exists(json_output_path):
        print(f"ðŸ“„ Se encontrÃ³ un archivo existente. Cargando progreso...")
        with open(json_output_path, mode='r', encoding='utf-8') as json_file:
            articles = json.load(json_file)
    else:
        print(f"ðŸ“„ No se encontrÃ³ progreso. Cargando desde el archivo base...")
        with open(json_input_path, mode='r', encoding='utf-8') as json_file:
            articles = json.load(json_file)

    print(f"âœ… Se cargaron {len(articles)} artÃ­culos.")

    articles_processed_since_start = 0
    for i, article in enumerate(articles):
        if article.get('texto_completo') and "No se pudo encontrar" not in article.get('texto_completo', ''):
            continue

        link = article.get('Link')
        if not link:
            article['texto_completo'] = "No se encontrÃ³ link."
            continue

        print(f"ðŸ”Ž Procesando artÃ­culo ID {article.get('id', i+1)}: {article.get('Title', 'Sin TÃ­tulo')[:40]}...")

        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
            response = requests.get(link, headers=headers, timeout=15)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')
            
            # --- Â¡NUEVA LÃ“GICA DE BÃšSQUEDA EN CASCADA! ---
            article_body = None

            # Intento 1: El selector original (el mÃ¡s comÃºn hasta ahora)
            article_body = soup.find('div', class_='j-article-body')

            # Intento 2: El selector especÃ­fico que descubriste
            if not article_body:
                article_body = soup.find('section', attrs={'aria-label': 'Article content'})

            # Intento 3: El selector mÃ¡s general como Ãºltimo recurso
            if not article_body:
                article_body = soup.find('article')
            
            if article_body:
                article['texto_completo'] = article_body.get_text(separator='\n', strip=True)
                print("  -> âœ… Texto extraÃ­do y guardado en memoria.")
            else:
                article['texto_completo'] = "No se pudo encontrar el cuerpo del artÃ­culo en la pÃ¡gina."

        except requests.exceptions.RequestException as e:
            print(f"âŒ Error al acceder al link para el artÃ­culo ID {article.get('id', i+1)}: {e}")
            article['texto_completo'] = "Error al intentar conectar con la URL."
        
        articles_processed_since_start += 1
        time.sleep(1)

        if articles_processed_since_start > 0 and articles_processed_since_start % 10 == 0:
            print(f"ðŸ’¾ Guardando progreso en disco... ({i+1}/{len(articles)})")
            with open(json_output_path, mode='w', encoding='utf-8') as json_file:
                json.dump(articles, json_file, indent=4, ensure_ascii=False)

    print("\nðŸŽ‰ Â¡Proceso completado! Guardando archivo final...")
    with open(json_output_path, mode='w', encoding='utf-8') as json_file:
        json.dump(articles, json_file, indent=4, ensure_ascii=False)
    print(f"âœ… Se ha guardado toda la informaciÃ³n en '{json_output_path}'.")

# --- Uso del script ---
json_base = 'salida.json' 
json_enriquecido = 'datos_texto_completo.json' 

enrich_json_with_scraping(json_base, json_enriquecido)